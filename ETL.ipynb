{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca461dfc",
   "metadata": {},
   "source": [
    "# Install and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "9719e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install(packages) and import_packages(packages) functions\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "def install(packages):\n",
    "    \"\"\"\n",
    "    Installs a Python packages with pip and gives a summary of the status.\n",
    "\n",
    "    Args:\n",
    "        packages (list of str): A list of package names to be installed.\n",
    "\n",
    "    The function attempts to install each package and summarizes the results:\n",
    "    - \"installed\": for successfully installed packages\n",
    "    - \"already installed\": for packages that are already installed\n",
    "    - \"failed\": for packages that couldn't be installed\n",
    "    - \"error at installing\": for packages where an installation error occurred\n",
    "    \"\"\"\n",
    "    \n",
    "    installed = []\n",
    "    already_installed = []\n",
    "    failed = []\n",
    "    errors = []\n",
    "\n",
    "    packages.sort() # Sort the packages to make the output more readable\n",
    "    \n",
    "    for package in packages: # Loop through packages and try to install them\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['pip', 'install', package],\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if \"Requirement already satisfied\" in result.stdout: #\n",
    "                already_installed.append(package)\n",
    "            elif \"Successfully installed\" in result.stdout:\n",
    "                installed.append(package)\n",
    "            else:\n",
    "                failed.append(package)\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error at installing {package}: {e}\")\n",
    "    \n",
    "    if installed:\n",
    "        print(f\"Installed: {', '.join(installed)}\")\n",
    "    if already_installed:\n",
    "        print(f\"Already installed: {', '.join(already_installed)}\")\n",
    "    if failed:\n",
    "        print(f\"Failed: {', '.join(failed)}\")\n",
    "    if errors:\n",
    "        print(\"\\n\".join(errors))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "def import_packages(packages):\n",
    "    \"\"\"\n",
    "    Imports a list of Python packages, assigning known aliases to some of them.\n",
    "\n",
    "    Args:\n",
    "        packages (list of str): A list of package names to be imported.\n",
    "\n",
    "    This function attempts to import each package in the provided list. If a package has a predefined alias \n",
    "    (e.g., `numpy` as `np` or `pandas` as `pd`), it will be imported with that alias. If no alias is defined, \n",
    "    the package will be imported with its original name. For each successful import, a message is printed. \n",
    "    If a package cannot be imported, an error message will indicate this and suggest checking if the package \n",
    "    is installed.\n",
    "    \"\"\"\n",
    "    aliases = {\"numpy\": \"np\", \"pandas\": \"pd\"}  # Dictionary of aliases\n",
    "    imports = []\n",
    "    for package in packages:\n",
    "        try:\n",
    "            if package in aliases:  # Import with alias\n",
    "                globals()[aliases[package]] = importlib.import_module(package)\n",
    "                print(f\"{package} imported as {aliases[package]}\")\n",
    "            else:  # Import without alias\n",
    "                globals()[package] = importlib.import_module(package)\n",
    "                print(f\"{package} imported\")\n",
    "            imports.append(package)\n",
    "        except ImportError:\n",
    "            print(f\"{package} couldn't be imported, is it installed?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "d727a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = [\"pandas\",\"gzip\",\"json\",\"ast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5917a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "install(packages)\n",
    "import_packages(packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0c5ee-b41a-4994-823f-d061db292caa",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "bdf3f242-78d1-40b4-b2b4-b774088c7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract function (supports CSV, JSON, and Gzip JSON)\n",
    "def extract(file_path):\n",
    "    \"\"\"\n",
    "    Extracts data from various file formats and returns it as a DataFrame.\n",
    "\n",
    "    Supported formats:\n",
    "    - CSV\n",
    "    - JSON (gzip compressed or plain)\n",
    "    - Other formats can be added in the future.\n",
    "\n",
    "    :param file_path: Path to the file.\n",
    "    :return: DataFrame with extracted data, or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine file format by extension\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(\"CSV data extraction successful.\\n\")\n",
    "        elif file_path.endswith('.json.gz'):\n",
    "            with gzip.open(file_path, 'rb') as file:\n",
    "                data = [json.loads(row) for row in file]\n",
    "            df = pd.DataFrame(data)\n",
    "            print(\"Gzip JSON data extraction successful.\\n\")\n",
    "        elif file_path.endswith('.json'):\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "            print(\"JSON data extraction successful.\\n\")\n",
    "        elif file_path.endswith('.ast.gz'):\n",
    "            with gzip.open(file_path, 'rb') as file:\n",
    "                data = [ast.literal_eval(row.decode('utf-8')) for row in file]\n",
    "            df = pd.DataFrame(data)\n",
    "            print(\"AST Gzip data extraction successful.\\n\")\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_path}\")\n",
    "            return None\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "5fd5dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_info function\n",
    "def df_info(df): #todo add docstring\n",
    "    # Obtain df name from global\n",
    "    df_name = [name for name, obj in globals().items() if obj is df][0]\n",
    "    # determine line length based on df name\n",
    "    line_length = max(64, len(df_name) + 10)\n",
    "    print(f\"\\n---Info-{df_name.replace(' ', '-')}{'-' * (line_length - len(df_name) - 10)}\\n\")\n",
    "    print(f\"---duplicated values:{df.duplicated().sum()}\")\n",
    "    print(f\"---Number of fully empty rows: {df.isnull().all(axis=1).sum()}\")\n",
    "    print(f\"\\n---Dataframe head:\\n{df.head(3).to_string()}\\n\")\n",
    "    print(f\"\\n---Dataframe info:\\n\")\n",
    "    print(df.info())\n",
    "    print(f\"\\n----Dataframe description:\\n{df.describe().to_string()}\")\n",
    "    print(f\"\\n---Missing values:\\n{df.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee34284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = extract(\"Raw data/train.csv\")\n",
    "df_test = extract(\"Raw data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c91e16",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "07aacc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_unique and unique_count functions\n",
    "def column_unique(df): #todo add docstring\n",
    "    for column in df.columns:\n",
    "        unique_values = df[column].unique()  # Get unique values in the column\n",
    "        \n",
    "        # Count the number of unique values\n",
    "        unique_count = len(unique_values)\n",
    "        \n",
    "        # Check if all values are different\n",
    "        if unique_count == len(df[column]):\n",
    "            print(f\"Column: {column}\")\n",
    "            print(f\"Unique values ({unique_count}): all values are different\")\n",
    "        else:\n",
    "            # Check if the column contains only numeric values\n",
    "            if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                sorted_values = sorted(unique_values)  # Sort numerically\n",
    "            else:\n",
    "                sorted_values = sorted(unique_values, key=str)  # Sort alphabetically if there are strings\n",
    "            \n",
    "            print(f\"Column: {column}\")\n",
    "            print(f\"Unique values ({unique_count}): {', '.join(map(str, sorted_values))}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def unique_count(df, column_name): #todo add docstring\n",
    "    if column_name in df.columns:\n",
    "        # Get the counts of unique elements, ordered by appearance\n",
    "        value_counts = df[column_name].value_counts()\n",
    "        \n",
    "        # Sort alphabetically for strings, or numerically for numbers\n",
    "        if pd.api.types.is_numeric_dtype(df[column_name]):\n",
    "            sorted_values = sorted(value_counts.items())  # Sort numerically by values\n",
    "        else:\n",
    "            sorted_values = sorted(value_counts.items(), key=lambda x: str(x[0]))  # Sort alphabetically by keys\n",
    "        \n",
    "        print(f\"\\nUnique values in column '{column_name}':\")\n",
    "        for value, count in sorted_values:\n",
    "            print(f\"{value}: {count} times\")\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' does not exist in the dataframe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ce721",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info(df_train)\n",
    "df_info(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_unique(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count(df_train, \"Survived\")\n",
    "unique_count(df_train, \"Pclass\")\n",
    "unique_count(df_train, \"Sex\")\n",
    "unique_count(df_train, \"Age\")\n",
    "unique_count(df_train, \"SibSp\")\n",
    "unique_count(df_train, \"Parch\")\n",
    "unique_count(df_train, \"Ticket\")\n",
    "unique_count(df_train, \"Fare\")\n",
    "unique_count(df_train, \"Cabin\")\n",
    "unique_count(df_train, \"Embarked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0566f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_unique(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dd5661",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count(df_test, \"PClass\")\n",
    "unique_count(df_test, \"Sex\")\n",
    "unique_count(df_test, \"Age\")\n",
    "unique_count(df_test, \"SibSp\")\n",
    "unique_count(df_test, \"Parch\")\n",
    "unique_count(df_test, \"Ticket\")\n",
    "unique_count(df_test, \"Fare\")\n",
    "unique_count(df_test, \"Cabin\")\n",
    "unique_count(df_test, \"Embarked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcda446-1268-4dd4-affe-9edbf2f98197",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "8d6360fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean function\n",
    "def df_clean(df): #todo add docstring\n",
    "    # Obtain df name from global\n",
    "    df_name = [name for name, obj in globals().items() if obj is df][0]\n",
    "    # determine line length based on df name\n",
    "    line_length = max(60, len(df_name) + 10)\n",
    "    print(f\"\\n---Cleaning-{df_name.replace(' ', '-')}{'-' * (line_length - len(df_name) - 10)}\\n\")\n",
    "    \n",
    "    # Print number of duplicated rows\n",
    "    duplicates_before = df.duplicated().sum()\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"{duplicates_before} duplicate rows removed.\")\n",
    "    \n",
    "    # Print number of missing values\n",
    "    missing_before = df.isnull().sum().sum()\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    print(f\"{missing_before} missing values removed.\")\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(\"Index reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4171169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean(df_train)\n",
    "df_clean(df_test)\n",
    "\n",
    "df_info(df_train)\n",
    "df_info(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e59693",
   "metadata": {},
   "source": [
    "## New rows creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19135854",
   "metadata": {},
   "source": [
    "## External Data enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9084d-3f9a-4e0e-988a-7c47c9c920ad",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "85f47f65-1619-4e26-849b-5874ba841e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('dataframes/df_train.csv', index=False, header=True)\n",
    "df_test.to_csv('dataframes/df_test.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
